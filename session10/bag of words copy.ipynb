{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-words example\n",
    "\n",
    "This notebook is a brief introduction to using bag of words. It is not meant as a guide to building a good Natural Language Processing network (it doesn't).\n",
    "\n",
    "The data used in this example is from Kaggle's Disaster Tweets: https://www.kaggle.com/competitions/nlp-getting-started/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "tweets = pd.read_csv('train.csv')\n",
    "\n",
    "tweets[tweets.target == 0].text.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target is 1, if the tweet is about a disaster, and 0 otherwise. We'll try to \n",
    "\n",
    "Since this is about NLP, I'll just use the text even though the keyword looks useful.\n",
    "\n",
    "The CountVectorizer finds all distinct words in the body of text (that is, all the rows). It returns a vector for each input text. The vector has a word count for how many times the word occured in the input text.\n",
    "\n",
    "Note the conversions to numpy arrays. Keras is none to happy with Pandas Dataframes.\n",
    "\n",
    "The shape of _X_ reveals that we have 7613 vectors (texts) and 21637 distinct words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "y = tweets.target.to_numpy()\n",
    "X = tweets.text.to_numpy()\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_, X_test, y_, y_test = train_test_split(X, y, train_size=.8, random_state=504)\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_, y_, train_size=.75, random_state=504)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "vocab_size = 5000\n",
    "embed_dim = 128\n",
    "\n",
    "vectorizationLayer = layers.TextVectorization(max_tokens=vocab_size)\n",
    "vectorizationLayer.adapt(X)\n",
    "vectorizationLayer(\"It really looks like it will rain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = tf.keras.Sequential([\n",
    "    vectorizationLayer,\n",
    "    layers.Embedding(vocab_size, embed_dim, mask_zero=True, embeddings_regularizer=tf.keras.regularizers.L1(.005)),\n",
    "    layers.GRU(8),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.002), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, start_from_epoch=25)\n",
    "lr = tf.keras.callbacks.ReduceLROnPlateau(patience=10, factor=.5, min_lr=1e-5)\n",
    "\n",
    "history = ann.fit(X_train, y_train, epochs = 100, validation_data=(X_validate, y_validate), callbacks=[es, lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "figure = plt.figure(figsize=(20, 10))\n",
    "ax = figure.add_subplot(1, 2, 1, title='Learning curves (loss)')\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.plot(history.history['loss'][5:], label = 'train')\n",
    "ax.plot(history.history['val_loss'][5:], label = 'valid')\n",
    "ax.legend()\n",
    "\n",
    "ax = figure.add_subplot(1, 2, 2, title='Learning curves (accuracy)')\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.plot(history.history['accuracy'][5:], label = 'train')\n",
    "ax.plot(history.history['val_accuracy'][5:], label = 'valid')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.evaluate(X_validate, y_validate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
