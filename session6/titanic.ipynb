{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from plot_utils import visualize_tree, plot_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('Titanic.csv')\n",
    "titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ready datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First clean up the data a little"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encode categorical features\n",
    "male_column = pd.get_dummies(titanic[\"Sex\"])[['male']]\n",
    "embark_columns = pd.get_dummies(titanic[\"Embarked\"])\n",
    "\n",
    "#replace categorical features with new features\n",
    "titanic = pd.concat([titanic, male_column, embark_columns], axis='columns').drop(['Sex', 'Embarked'], axis='columns')\n",
    "\n",
    "#drop non-categorical text features\n",
    "titanic = titanic.drop(['Name', 'Ticket', 'Cabin'], axis='columns')\n",
    "\n",
    "#drop data with NaN values\n",
    "titanic = titanic.dropna()\n",
    "\n",
    "titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = titanic['Survived']\n",
    "X = titanic.drop('Survived', axis='columns')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=504)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(random_state=504)\n",
    "tree_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 6>What could possibly go wrong?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like overfitting. How about the test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tree(tree_clf, X_train.columns, [\"Died\", \"Survived\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PassengerId should be irrelevant, but it allows the decision tree to turn into a full binary search tree. Let's drop it.\n",
    "\n",
    "Also, Fare seems correlated to Pclass. Let's drop that, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_train, X_test = X.drop(['PassengerId', 'Fare'], axis='columns'), X_train.drop(\"PassengerId\", axis='columns'), X_test.drop(\"PassengerId\", axis='columns')\n",
    "\n",
    "tree_clf.fit(X_train, y_train)\n",
    "tree_clf.score(X_train, y_train), tree_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tree(tree_clf, X_train.columns, [\"Died\", \"Survived\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unconstrained decision trees tend to overfit, because they end up close to full binary search trees. This is also known as _pre-pruning_\n",
    "We can constrain decision trees in several ways. Let's look at three important hyper-parameters.\n",
    "* max_depth - puts a limit on how deep the tree can get. *Note*: The tree must be deep enough to accomodate all classes in the classification: 2^depth >= classes\n",
    "* min_samples_split - forces a minimum of samples in a node before it gets split into subnodes\n",
    "* max_features - forces the algorithm to only consider a certain number of features per node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier(random_state=504, max_depth=3)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "tree_clf.score(X_train, y_train), tree_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tree(tree_clf, X_train.columns, [\"Died\", \"Survived\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This decision tree is much less likely to overfit, because it is forced to generalized more. Of course, if the tree is too shallow, it will underfit.\n",
    "\n",
    "Let's see how under- and overfitting depends on tree depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = range(1, 11)\n",
    "\n",
    "classifiers = [DecisionTreeClassifier(random_state=504, max_depth=d) for d in features]\n",
    "for clf in classifiers: clf.fit(X_train, y_train)\n",
    "\n",
    "train_scores = [clf.score(X_train, y_train) for clf in classifiers]\n",
    "test_scores = [clf.score(X_test, y_test) for clf in classifiers]\n",
    "plot_results(train_scores, test_scores, train_label=\"Train accuracy\", test_label=\"Test accuracy\", xlabel=\"depth\", ylabel=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This illustrates quite clearly how dangerous overfitting is for decision trees. Anyway, looks like a depth of 3 is the sweet spot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min_samples_split\n",
    "Another way of constraining the tree is to be more reluctant to split. For instance, don't split further if there are no more than 10 samples in a leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier(random_state=504, min_samples_split=10)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "tree_clf.score(X_train, y_train), tree_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tree(tree_clf, X_train.columns, [\"Died\", \"Survived\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the tree performs on different minimal node sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = range(2, 31)\n",
    "\n",
    "classifiers = [DecisionTreeClassifier(random_state=504, min_samples_split=s) for s in sizes]\n",
    "for clf in classifiers: clf.fit(X_train, y_train)\n",
    "\n",
    "train_scores = [clf.score(X_train, y_train) for clf in classifiers]\n",
    "test_scores = [clf.score(X_test, y_test) for clf in classifiers]\n",
    "plot_results(train_scores, test_scores, train_label=\"Train accuracy\", test_label=\"Test accuracy\", xlabel=\"min split size\", ylabel=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tree(tree_clf, X_train.columns, [\"Died\", \"Survived\"]).view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A shallow tree will usually have rather large samples in the leaves, so it doesn't make much sense to mix max_depth and min_samples_split.\n",
    "\n",
    "An alternative way of telling the algorithm \"only split if it's worth it\" is to forbid splitting a node unless the improvement is big enough: min_impurity_decrease (see visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max_features\n",
    "\n",
    "This is an odd duck. At each node the algorithm chooses a random subset of the features, which are used to evaluate the best split. It does *not* mean 'only split on the best n features', it means 'split on n *random* features'.\n",
    "\n",
    "This makes for a faster algorithm if we have many features, but it also introduces random bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = range(1, 10)\n",
    "\n",
    "classifiers = [DecisionTreeClassifier(random_state=504, max_features=f, max_depth=3) for f in features]\n",
    "for clf in classifiers: clf.fit(X_train, y_train)\n",
    "\n",
    "train_scores = [clf.score(X_train, y_train) for clf in classifiers]\n",
    "test_scores = [clf.score(X_test, y_test) for clf in classifiers]\n",
    "plot_results(train_scores, test_scores, train_label=\"Train accuracy\", test_label=\"Test accuracy\", xlabel=\"min split size\", ylabel=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, max_features should be 4. \n",
    "\n",
    "**What am I doing wrong here?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier(random_state=504, max_features=4, max_depth=3)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "tree_clf.score(X_train, y_train), tree_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this looks nice and not at all overfitting, we are tuning the hyperparameters on the test set. Now we need another set for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us instead search for the best solution using cross validation. The nice thing about cross validation is that it doesn't use the test set for validation but part of the train set. So we can still test for overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "depths = range(1, 11)\n",
    "sizes = range(2, 21)\n",
    "features = range(1, 9)\n",
    "params = {'min_samples_split': sizes, \n",
    "          'max_depth': depths, \n",
    "          'max_features': features}\n",
    "\n",
    "gsc = GridSearchCV(DecisionTreeClassifier(random_state=504), params, n_jobs=16)\n",
    "gsc.fit(X_train, y_train)\n",
    "gsc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsc.score(X_train, y_train), gsc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems good, but try it with some different random seeds. There is a lot of noise here, and it's probably best to keep it simple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = zip(X_train.keys(), gsc.best_estimator_.feature_importances_) #pairs up feature names with performance score\n",
    "sorted(importances, key=lambda p: -p[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
