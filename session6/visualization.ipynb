{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble\n",
    "\n",
    "importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "from plot_utils import visualize_tree, grid_plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers=[[-5, 0], [5, 5], [5, -5]]\n",
    "blobs = make_blobs(centers=centers, cluster_std= 2.5, n_samples=300, random_state=504)\n",
    "\n",
    "X, y = blobs\n",
    "\n",
    "colors = ListedColormap([\"green\", \"yellow\", \"magenta\"])\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a decision tree\n",
    "\n",
    "Creating and fitting on the data. I didn't create train and test sets. This is only for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(random_state=504, min_impurity_decrease=.02)\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tree(tree_clf, [\"X_1\", \"X_2\"], [\"Green\", \"Yellow\", \"Magenta\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The position in the center is predicted as magenta. You can also see this by following the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf.predict([[0, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree is quite clean, but of course it can't be perfect. We can check the probabilities of the predictions. These are proportional to the number of samples of the category in the leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf.predict_proba([[0, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows how the decision tree bisects the sample space into rectangles. Note that all lines are horizontal or vertical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionBoundaryDisplay.from_estimator(tree_clf, X, alpha=.2, cmap=colors, xlabel='X_1', ylabel='X_2')\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=colors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A harder example\n",
    "\n",
    "Let's try creating a harder example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs = make_blobs(centers=[[-2.5, 0], [2.5, 4], [8.5, -1.5]], cluster_std= 2.5, n_samples=300, random_state=504)\n",
    "\n",
    "X, y = blobs\n",
    "\n",
    "colors = ListedColormap([\"green\", \"yellow\", \"magenta\"])\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tree_clf = DecisionTreeClassifier(random_state=504, min_impurity_decrease=.02)\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this is a lot more complex even without changing the hyperparameters. The bisection of the sample space is also a lot more complex and seems to have weird artifacts not in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tree(tree_clf, [\"X_1\", \"X_2\"], [\"Green\", \"Yellow\", \"Magenta\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionBoundaryDisplay.from_estimator(tree_clf, X, alpha=.2, cmap=colors, xlabel='X_1', ylabel='X_2')\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=colors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting\n",
    "\n",
    "Let's try tuning the hyperparameters to get a more precise result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tree_clf = DecisionTreeClassifier(random_state=504)\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tree(tree_clf, [\"X_1\", \"X_2\"], [\"Green\", \"Yellow\", \"Magenta\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionBoundaryDisplay.from_estimator(tree_clf, X, alpha=.2, cmap=colors, xlabel='X_1', ylabel='X_2')\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=colors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even without a test set, we can clearly see the overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest\n",
    "\n",
    "Let's create a random forest to tackle the problem. We use 15 trees in the forest, which is low, but good for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_clf = RandomForestClassifier(random_state=504, min_impurity_decrease=.02, n_estimators=15, max_features=2)\n",
    "forest_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = forest_clf.estimators_\n",
    "\n",
    "def plot(ax, model, title):\n",
    "    ax.set_title(title)\n",
    "    DecisionBoundaryDisplay.from_estimator(model, X, ax=ax, alpha=.2, cmap=colors)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=colors)\n",
    "\n",
    "def tree_plot(ax, tree, index):\n",
    "    plot(ax, tree, \"tree {}\".format(index + 1))\n",
    "\n",
    "axes = grid_plot(trees, 4, 4, tree_plot)\n",
    "plot(axes[3][3], forest_clf, \"Forest\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the forest is better than any individual attempt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
